# Pipeline Modificado - Guia de Uso

## ğŸ”„ Novo Fluxo do Pipeline

O pipeline foi reorganizado para incluir validaÃ§Ã£o robusta de CSV e prevenÃ§Ã£o de corrupÃ§Ã£o de dados:

### Pipeline Steps:
1. **ğŸ“¥ Download ZIP data and extract to CSV** - Download com verificaÃ§Ã£o CHECKSUM + extraÃ§Ã£o
2. **ğŸ” Validate CSV integrity and convert to Parquet** - ValidaÃ§Ã£o de CSV + conversÃ£o segura
3. **ğŸ”§ Optimize Parquet files** - OtimizaÃ§Ã£o robusta com prevenÃ§Ã£o de corrupÃ§Ã£o
4. **âœ… Validate optimized data integrity** - ValidaÃ§Ã£o final dos dados otimizados
5. **ğŸ“Š Generate features** - GeraÃ§Ã£o de features para ML
6. **ğŸšª Exit**

## ğŸ“‹ Detalhes de Cada Etapa

### Step 1: Download e ExtraÃ§Ã£o
**O que faz:**
- Downloads paralelos com verificaÃ§Ã£o SHA256/CHECKSUM
- ExtraÃ§Ã£o automÃ¡tica de todos os arquivos .zip para .csv
- VerificaÃ§Ã£o de integridade dos CSV extraÃ­dos
- OpÃ§Ã£o de limpeza automÃ¡tica dos ZIPs apÃ³s extraÃ§Ã£o bem-sucedida

**Melhorias:**
- âœ… VerificaÃ§Ã£o de checksum obrigatÃ³ria
- âœ… ExtraÃ§Ã£o integrada no processo
- âœ… ValidaÃ§Ã£o bÃ¡sica dos CSV extraÃ­dos
- âœ… Rastreamento de progresso para downloads e extraÃ§Ãµes

### Step 2: ValidaÃ§Ã£o de CSV e ConversÃ£o
**O que faz:**
- ValidaÃ§Ã£o detalhada da integridade dos arquivos CSV
- DetecÃ§Ã£o automÃ¡tica de formato (com/sem header)
- VerificaÃ§Ã£o de:
  - Colunas obrigatÃ³rias (time, price, qty)
  - Valores nulos em colunas crÃ­ticas
  - PreÃ§os invÃ¡lidos (â‰¤0)
  - Formato de timestamp
- ConversÃ£o segura CSV â†’ Parquet com tipos otimizados
- VerificaÃ§Ã£o dos arquivos Parquet gerados

**Melhorias:**
- ğŸ›¡ï¸ ValidaÃ§Ã£o abrangente antes da conversÃ£o
- ğŸ” DetecÃ§Ã£o de problemas de dados
- ğŸ“Š RelatÃ³rios detalhados de qualidade
- âœ… VerificaÃ§Ã£o pÃ³s-conversÃ£o

### Step 3: OtimizaÃ§Ã£o de Parquet
**O que faz:**
- Combina arquivos pequenos em chunks maiores (padrÃ£o 10GB)
- MantÃ©m ordem cronolÃ³gica dos dados
- Otimiza para melhor performance de I/O
- Cleanup automÃ¡tico em caso de erro

**Melhorias:**
- ğŸ”„ Processamento eficiente de arquivos grandes
- ğŸ“‹ Logs detalhados de cada operaÃ§Ã£o
- âœ… VerificaÃ§Ã£o de dados apÃ³s otimizaÃ§Ã£o
- ğŸ’¾ ReduÃ§Ã£o do nÃºmero de arquivos

### Step 4: ValidaÃ§Ã£o Final
**OpÃ§Ãµes disponÃ­veis:**
1. Quick validation - VerificaÃ§Ã£o rÃ¡pida bÃ¡sica
2. Advanced validation - RelatÃ³rios detalhados
3. Missing dates validation - VerificaÃ§Ã£o de gaps temporais
4. **ğŸ›¡ï¸ Comprehensive integrity validation** (NOVO) - ValidaÃ§Ã£o completa com score de qualidade

**Melhorias:**
- ğŸ“Š Score de qualidade de dados (0-100)
- ğŸ” DetecÃ§Ã£o de anomalias com Numba
- ğŸ“„ RelatÃ³rios JSON detalhados
- âš¡ Processamento paralelo

### Step 5: GeraÃ§Ã£o de Features
- Permanece inalterado
- GeraÃ§Ã£o de imbalance dollar bars
- Processamento com Dask distribuÃ­do

## ğŸš€ Como Usar

### ExecuÃ§Ã£o Interativa
```bash
python main.py
```

### ExecuÃ§Ã£o em Lote
```bash
# Exemplo completo para BTCUSDT spot monthly
python main.py download --symbol BTCUSDT --type spot --granularity monthly --start 2024-01 --end 2024-03 --workers 5

# ValidaÃ§Ã£o e conversÃ£o de CSV (nova funcionalidade)
python src/data_pipeline/converters/csv_to_parquet.py --symbol BTCUSDT --type spot --granularity monthly --cleanup --verify

# OtimizaÃ§Ã£o de parquet
python src/data_pipeline/processors/parquet_optimizer.py --source datasets/dataset-raw-monthly-compressed/spot --target datasets/dataset-raw-monthly-compressed-optimized/spot --max-size 10

# ValidaÃ§Ã£o integral
python src/data_pipeline/validators/data_integrity_validator.py --directory datasets/dataset-raw-monthly-compressed-optimized/spot --output reports/integrity_report.json --verbose
```

## ğŸ›¡ï¸ BenefÃ­cios do Novo Pipeline

### PrevenÃ§Ã£o de CorrupÃ§Ã£o
- **ValidaÃ§Ã£o em mÃºltiplas camadas**: CSV â†’ Parquet â†’ Otimizado
- **Checksums e verificaÃ§Ãµes**: Em cada etapa crÃ­tica
- **Fail-safe mechanisms**: Rollback automÃ¡tico em caso de erro
- **Arquivos temporÃ¡rios**: OperaÃ§Ãµes seguras com staging

### Observabilidade
- **Logs detalhados**: Rastreamento completo de operaÃ§Ãµes
- **MÃ©tricas de qualidade**: Score e relatÃ³rios de integridade
- **Progress tracking**: Estado persistente para retomar operaÃ§Ãµes
- **RelatÃ³rios JSON**: Dados estruturados para anÃ¡lise

### Performance
- **Processamento Numba**: OperaÃ§Ãµes crÃ­ticas otimizadas
- **ParalelizaÃ§Ã£o**: Downloads e validaÃ§Ãµes concorrentes
- **Batch processing**: OperaÃ§Ãµes em lote eficientes
- **Memory efficient**: Streaming para arquivos grandes

## âš ï¸ Notas Importantes

1. **Sempre execute as etapas em ordem**: O pipeline foi projetado para ser sequencial
2. **Verifique logs em caso de erro**: Logs detalhados estÃ£o em `datasets/logs/`
3. **Use modo robusto por padrÃ£o**: Especialmente para dados crÃ­ticos
4. **Mantenha backups**: O sistema pode criar backups automÃ¡ticos se configurado
5. **Monitore o espaÃ§o em disco**: ValidaÃ§Ãµes podem usar espaÃ§o temporÃ¡rio adicional

## ğŸ”§ Troubleshooting

### Problema: CSV com formato inconsistente
**SoluÃ§Ã£o**: Use a validaÃ§Ã£o da Etapa 2 para identificar e corrigir problemas

### Problema: Arquivos Parquet corrompidos
**SoluÃ§Ã£o**: Use o otimizador robusto (Etapa 3) que detecta e previne corrupÃ§Ã£o

### Problema: Dados faltando apÃ³s otimizaÃ§Ã£o
**SoluÃ§Ã£o**: VerificaÃ§Ã£o automÃ¡tica de row count e checksums previne perda de dados

### Problema: Performance lenta
**SoluÃ§Ã£o**: Ajuste o nÃºmero de workers e use processamento em lote